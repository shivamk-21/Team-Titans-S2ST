{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9791f8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c88477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "SPLITS = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "def find_audio_file(useable_root: Path, src_audio: str) -> Tuple[Path, bool]:\n",
    "    p = Path(src_audio)\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p, True\n",
    "    cand = useable_root / p\n",
    "    if cand.exists():\n",
    "        return cand, True\n",
    "    cand2 = useable_root / p.name\n",
    "    if cand2.exists():\n",
    "        return cand2, True\n",
    "    if len(p.parts) > 1:\n",
    "        cand3 = useable_root / p.parts[0] / p.name\n",
    "        if cand3.exists():\n",
    "            return cand3, True\n",
    "    return cand2, cand2.exists()\n",
    "\n",
    "def _read_tsv_rows(useable_root: Path, split: str) -> List[Dict[str, str]]:\n",
    "    tsv = useable_root / f\"{split}.tsv\"\n",
    "    if not tsv.exists():\n",
    "        return []\n",
    "    rows = []\n",
    "    with tsv.open(\"r\", encoding=\"utf-8\", newline=\"\") as fh:\n",
    "        reader = csv.DictReader(fh, delimiter=\"\\t\")\n",
    "        for r in reader:\n",
    "            src = (r.get(\"src_audio\") or \"\").strip()\n",
    "            if not src:\n",
    "                continue\n",
    "            lang = (r.get(\"language\") or \"\").strip()\n",
    "            rows.append({\"src_audio\": src, \"language\": lang})\n",
    "    return rows\n",
    "\n",
    "def _prefix_from_src(src: str, language: str, covost_mode: bool=False) -> str:\n",
    "    p = Path(src)\n",
    "    if covost_mode and len(p.parts) >= 1 and p.parts[0]:\n",
    "        return p.parts[0]\n",
    "    if len(p.parts) > 1:\n",
    "        return p.parts[0]\n",
    "    return language or \"unknown\"\n",
    "\n",
    "def _lang_folder(prefix: str) -> str:\n",
    "    return prefix.replace(\"_\", \"-\")\n",
    "\n",
    "def _worker_check_file(\n",
    "    task: Tuple[str, str, Dict[str,str]],\n",
    "    useable_root_p: Path,\n",
    "    out_root_p: Path\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Worker receives (prefix, split, row) and returns a small dict:\n",
    "      {\n",
    "        'prefix': prefix,\n",
    "        'split': split,\n",
    "        'basename': basename,\n",
    "        'src_exists': bool,\n",
    "        'src_tried': str,\n",
    "        'dest_exists': bool,\n",
    "        'dest_path': str\n",
    "      }\n",
    "    \"\"\"\n",
    "    pref, split, row = task\n",
    "    src_field = row[\"src_audio\"]\n",
    "    basename = Path(src_field).name\n",
    "    src_candidate, src_exists = find_audio_file(useable_root_p, src_field)\n",
    "    dest_path = out_root_p / _lang_folder(pref) / split / basename\n",
    "    dest_exists = dest_path.exists()\n",
    "    return {\n",
    "        \"prefix\": pref,\n",
    "        \"split\": split,\n",
    "        \"basename\": basename,\n",
    "        \"src_field\": src_field,\n",
    "        \"src_tried\": str(src_candidate),\n",
    "        \"src_exists\": src_exists,\n",
    "        \"dest_path\": str(dest_path),\n",
    "        \"dest_exists\": dest_exists,\n",
    "    }\n",
    "\n",
    "def verify_cvss_copy_streaming(\n",
    "    useable_root: str,\n",
    "    out_root: str,\n",
    "    filter_language: Optional[str] = None,\n",
    "    covost_mode: bool = False,\n",
    "    max_workers: int = 8,\n",
    "    show_progress: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Streaming verifier: per-row checks with a single tqdm progress bar.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"groups\": { (prefix,split) : { aggregated info } },\n",
    "        \"summary\": {...}\n",
    "      }\n",
    "    \"\"\"\n",
    "    useable_root_p = Path(useable_root).absolute()\n",
    "    out_root_p = Path(out_root).absolute()\n",
    "\n",
    "    # 1. Load all rows and build tasks list (prefix, split, row)\n",
    "    tasks: List[Tuple[str, str, Dict[str,str]]] = []\n",
    "    prefixes = set()\n",
    "    for split in SPLITS:\n",
    "        rows = _read_tsv_rows(useable_root_p, split)\n",
    "        for r in rows:\n",
    "            pref = _prefix_from_src(r[\"src_audio\"], r.get(\"language\",\"\"), covost_mode=covost_mode)\n",
    "            prefixes.add(pref)\n",
    "            tasks.append((pref, split, r))\n",
    "\n",
    "    # apply filter_language\n",
    "    if filter_language is not None:\n",
    "        def _match(p: str, flt: str) -> bool:\n",
    "            return p == flt or p.replace(\"_\",\"-\") == flt or (flt in p) or (flt in p.replace(\"_\",\"-\"))\n",
    "        tasks = [t for t in tasks if _match(t[0], filter_language)]\n",
    "        prefixes = {t[0] for t in tasks}\n",
    "        if not tasks:\n",
    "            return {\"error\": f\"no tasks after applying filter_language='{filter_language}'\", \"groups\": {}}\n",
    "\n",
    "    total_tasks = len(tasks)\n",
    "    if total_tasks == 0:\n",
    "        return {\"error\": \"no tasks found (check your useable_root *.tsv files)\", \"groups\": {}}\n",
    "\n",
    "    # 2. Stream-check using executor.map and tqdm\n",
    "    groups_agg: Dict[Tuple[str,str], Dict[str, Any]] = {}\n",
    "    # initialize groups\n",
    "    for p in prefixes:\n",
    "        for s in SPLITS:\n",
    "            groups_agg[(p,s)] = {\n",
    "                \"prefix\": p,\n",
    "                \"split\": s,\n",
    "                \"expected_rows\": 0,\n",
    "                \"found_on_dest\": 0,\n",
    "                \"missing_dest\": [],    # sample upto a few\n",
    "                \"missing_src\": [],\n",
    "                \"duplicates_counter\": Counter(),\n",
    "            }\n",
    "\n",
    "    # count expected_rows and duplicates_counter first to fill duplicates info\n",
    "    for pref, split, row in tasks:\n",
    "        key = (pref, split)\n",
    "        groups_agg[key][\"expected_rows\"] += 1\n",
    "        basename = Path(row[\"src_audio\"]).name\n",
    "        groups_agg[key][\"duplicates_counter\"][basename] += 1\n",
    "\n",
    "    # Worker wrapper for mapping: it needs only task and global roots\n",
    "    def _map_iter():\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            for result in ex.map(lambda t: _worker_check_file(t, useable_root_p, out_root_p), tasks, chunksize=256):\n",
    "                yield result\n",
    "\n",
    "    iterator = _map_iter()\n",
    "    if show_progress:\n",
    "        iterator = tqdm(iterator, total=total_tasks, desc=\"verifying files\", unit=\"file\")\n",
    "\n",
    "    # stream results and aggregate\n",
    "    for res in iterator:\n",
    "        key = (res[\"prefix\"], res[\"split\"])\n",
    "        g = groups_agg[key]\n",
    "        if res[\"dest_exists\"]:\n",
    "            g[\"found_on_dest\"] += 1\n",
    "        else:\n",
    "            if len(g[\"missing_dest\"]) < 100:  # keep a reasonable sample\n",
    "                g[\"missing_dest\"].append({\"basename\": res[\"basename\"], \"expected_dest\": res[\"dest_path\"], \"src_exists\": res[\"src_exists\"], \"src_tried\": res[\"src_tried\"]})\n",
    "        if not res[\"src_exists\"]:\n",
    "            if len(g[\"missing_src\"]) < 100:\n",
    "                g[\"missing_src\"].append({\"src_field\": res[\"src_field\"], \"tried\": res[\"src_tried\"]})\n",
    "\n",
    "    # finalize groups: compute duplicates list and counts\n",
    "    final_groups = {}\n",
    "    total_expected = 0\n",
    "    total_found = 0\n",
    "    total_missing_dest = 0\n",
    "    total_missing_src = 0\n",
    "    groups_with_problems = 0\n",
    "    for key, g in groups_agg.items():\n",
    "        dup_list = [b for b,c in g[\"duplicates_counter\"].items() if c > 1]\n",
    "        missing_dest_count = sum(1 for _ in range(g[\"expected_rows\"]) ) if False else len(g[\"_dummy\"]) if False else None\n",
    "        # we didn't store per-row existence counts per-file other than found_on_dest\n",
    "        # use stored values:\n",
    "        missing_dest_count = g[\"expected_rows\"] - g[\"found_on_dest\"]\n",
    "        missing_src_count = len(g[\"missing_src\"])\n",
    "        total_expected += g[\"expected_rows\"]\n",
    "        total_found += g[\"found_on_dest\"]\n",
    "        total_missing_dest += missing_dest_count\n",
    "        total_missing_src += missing_src_count\n",
    "        if missing_dest_count or missing_src_count or dup_list:\n",
    "            groups_with_problems += 1\n",
    "        final_groups[key] = {\n",
    "            \"prefix\": g[\"prefix\"],\n",
    "            \"split\": g[\"split\"],\n",
    "            \"expected_rows\": g[\"expected_rows\"],\n",
    "            \"found_on_dest\": g[\"found_on_dest\"],\n",
    "            \"missing_dest_count\": missing_dest_count,\n",
    "            \"missing_dest_sample\": g[\"missing_dest\"],\n",
    "            \"missing_src_count\": missing_src_count,\n",
    "            \"missing_src_sample\": g[\"missing_src\"],\n",
    "            \"duplicates_count\": len(dup_list),\n",
    "            \"duplicates_sample\": dup_list[:50],\n",
    "        }\n",
    "\n",
    "    summary = {\n",
    "        \"useable_root\": str(useable_root_p),\n",
    "        \"out_root\": str(out_root_p),\n",
    "        \"covost_mode\": covost_mode,\n",
    "        \"total_tasks\": total_tasks,\n",
    "        \"total_expected\": total_expected,\n",
    "        \"total_found_on_dest\": total_found,\n",
    "        \"total_missing_on_dest\": total_missing_dest,\n",
    "        \"total_missing_sources\": total_missing_src,\n",
    "        \"groups_with_problems\": groups_with_problems,\n",
    "    }\n",
    "\n",
    "    # concise print\n",
    "    print(\"==== verify_cvss_copy_streaming summary ====\")\n",
    "    print(f\"Total files checked: {summary['total_tasks']}\")\n",
    "    print(f\"Total expected: {summary['total_expected']}, found on dest: {summary['total_found_on_dest']}, missing on dest: {summary['total_missing_on_dest']}\")\n",
    "    print(f\"Missing sources: {summary['total_missing_sources']}\")\n",
    "    if summary[\"groups_with_problems\"] == 0:\n",
    "        print(\"All good ✅\")\n",
    "    else:\n",
    "        print(f\"Issues in {summary['groups_with_problems']} group(s). Inspect 'groups' for details.\")\n",
    "\n",
    "    return {\"groups\": final_groups, \"summary\": summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3ad8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "verifying files: 100%|██████████| 210309/210309 [1:08:12<00:00, 51.39file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== verify_cvss_copy_streaming summary ====\n",
      "Total files checked: 210309\n",
      "Total expected: 210309, found on dest: 210309, missing on dest: 0\n",
      "Missing sources: 0\n",
      "All good ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# normal run (parallel, auto progress)\n",
    "res = verify_cvss_copy_streaming(\"F:\\ML-Project\\Datasets\\original\",\"F:\\ML-Project\\Datasets\\processed_datasets\\cvss\\cvss-c\", max_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f573db4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "verifying files: 100%|██████████| 210309/210309 [54:32<00:00, 64.26file/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== verify_cvss_copy_streaming summary ====\n",
      "Total files checked: 210309\n",
      "Total expected: 210309, found on dest: 0, missing on dest: 210309\n",
      "Missing sources: 0\n",
      "Issues in 9 group(s). Inspect 'groups' for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_covost = verify_cvss_copy_streaming(\"F:\\ML-Project\\Datasets\\original\", \"F:\\ML-Project\\Datasets\\processed_datasets\\covost\", covost_mode=True, max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3682a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "SPLITS = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "def _list_files_fast(dirpath: Path) -> List[str]:\n",
    "    if not dirpath.exists() or not dirpath.is_dir():\n",
    "        return []\n",
    "    try:\n",
    "        return [ent.name for ent in os.scandir(dirpath) if ent.is_file()]\n",
    "    except Exception:\n",
    "        return [p.name for p in dirpath.iterdir() if p.is_file()]\n",
    "\n",
    "def _read_tsv_rows(useable_root: Path, split: str) -> List[Dict[str,str]]:\n",
    "    tsv = useable_root / f\"{split}.tsv\"\n",
    "    if not tsv.exists():\n",
    "        return []\n",
    "    rows = []\n",
    "    with tsv.open(\"r\", encoding=\"utf-8\", newline=\"\") as fh:\n",
    "        reader = csv.DictReader(fh, delimiter=\"\\t\")\n",
    "        for r in reader:\n",
    "            src = (r.get(\"src_audio\") or \"\").strip()\n",
    "            if not src:\n",
    "                continue\n",
    "            lang = (r.get(\"language\") or \"\").strip()\n",
    "            rows.append({\"src_audio\": src, \"language\": lang})\n",
    "    return rows\n",
    "\n",
    "def _lang_folder(prefix: str, lang_map: Optional[Dict[str,str]]=None) -> str:\n",
    "    if lang_map and prefix in lang_map:\n",
    "        return lang_map[prefix].replace(\"_\",\"-\")\n",
    "    return prefix.replace(\"_\",\"-\")\n",
    "\n",
    "def _find_audio_covost(useable_root: Path, src_audio: str, recursive_search_depth: int = 2) -> Tuple[Path, bool]:\n",
    "    \"\"\"\n",
    "    CoVoST-aware lookup for a src_audio like 'en/speaker/file.wav' or 'en/file.wav'\n",
    "    Tries:\n",
    "      1) If absolute path and exists -> return it\n",
    "      2) useable_root / lang_code / basename\n",
    "      3) useable_root / lang_code / */ basename  (one-level speaker subdir)\n",
    "      4) optionally bounded recursive search under useable_root/lang_code up to depth\n",
    "      5) fallback: useable_root / basename (not found)\n",
    "    Returns (candidate_path, exists_bool)\n",
    "    \"\"\"\n",
    "    p = Path(src_audio)\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p, True\n",
    "    if len(p.parts) == 0:\n",
    "        cand = useable_root / p.name\n",
    "        return cand, cand.exists()\n",
    "\n",
    "    lang_code = p.parts[0]\n",
    "    basename = p.name\n",
    "\n",
    "    # 2) direct under lang_code\n",
    "    cand = useable_root / lang_code / basename\n",
    "    if cand.exists():\n",
    "        return cand, True\n",
    "    # 3) one-level speaker subdir\n",
    "    lang_dir = useable_root / lang_code\n",
    "    if lang_dir.exists() and lang_dir.is_dir():\n",
    "        try:\n",
    "            for entry in os.scandir(lang_dir):\n",
    "                if not entry.is_dir():\n",
    "                    continue\n",
    "                cand2 = Path(entry.path) / basename\n",
    "                if cand2.exists():\n",
    "                    return cand2, True\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 4) bounded depth recursive search (avoid deep full walk)\n",
    "    if lang_dir.exists() and lang_dir.is_dir() and recursive_search_depth > 0:\n",
    "        # BFS style traversal up to depth\n",
    "        from collections import deque\n",
    "        q = deque([(lang_dir, 0)])\n",
    "        while q:\n",
    "            cur_dir, depth = q.popleft()\n",
    "            if depth > recursive_search_depth:\n",
    "                continue\n",
    "            try:\n",
    "                for entry in os.scandir(cur_dir):\n",
    "                    if entry.is_file() and entry.name == basename:\n",
    "                        return Path(entry.path), True\n",
    "                    if entry.is_dir():\n",
    "                        q.append((Path(entry.path), depth+1))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 5) fallback: try useable_root/basename\n",
    "    cand_fallback = useable_root / basename\n",
    "    return cand_fallback, cand_fallback.exists()\n",
    "\n",
    "def _find_audio_general(useable_root: Path, src_audio: str) -> Tuple[Path,bool]:\n",
    "    p = Path(src_audio)\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p, True\n",
    "    cand = useable_root / p\n",
    "    if cand.exists():\n",
    "        return cand, True\n",
    "    cand2 = useable_root / p.name\n",
    "    if cand2.exists():\n",
    "        return cand2, True\n",
    "    if len(p.parts) > 1:\n",
    "        cand3 = useable_root / p.parts[0] / p.name\n",
    "        if cand3.exists():\n",
    "            return cand3, True\n",
    "    return cand2, cand2.exists()\n",
    "\n",
    "def _worker_covost_task(\n",
    "    task: Tuple[str, str, Dict[str,str]],\n",
    "    useable_root_p: Path,\n",
    "    out_root_p: Path,\n",
    "    covost_mode: bool,\n",
    "    lang_map: Optional[Dict[str,str]],\n",
    "    recursive_search_depth: int\n",
    ") -> Dict[str, Any]:\n",
    "    pref, split, row = task\n",
    "    src_field = row[\"src_audio\"]\n",
    "    basename = Path(src_field).name\n",
    "\n",
    "    if covost_mode:\n",
    "        src_candidate, src_exists = _find_audio_covost(useable_root_p, src_field, recursive_search_depth=recursive_search_depth)\n",
    "    else:\n",
    "        src_candidate, src_exists = _find_audio_general(useable_root_p, src_field)\n",
    "\n",
    "    dest_path = out_root_p / _lang_folder(pref, lang_map) / split / basename\n",
    "    dest_exists = dest_path.exists()\n",
    "\n",
    "    return {\n",
    "        \"prefix\": pref,\n",
    "        \"split\": split,\n",
    "        \"basename\": basename,\n",
    "        \"src_field\": src_field,\n",
    "        \"src_tried\": str(src_candidate),\n",
    "        \"src_exists\": src_exists,\n",
    "        \"dest_path\": str(dest_path),\n",
    "        \"dest_exists\": dest_exists,\n",
    "    }\n",
    "\n",
    "def verify_cvss_copy_covost(\n",
    "    useable_root: str,\n",
    "    out_root: str,\n",
    "    filter_language: Optional[str] = None,\n",
    "    covost_mode: bool = False,\n",
    "    lang_map: Optional[Dict[str,str]] = None,\n",
    "    max_workers: int = 8,\n",
    "    show_progress: bool = True,\n",
    "    recursive_search_depth: int = 2\n",
    ") -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    Streaming verifier that supports CoVoST layout.\n",
    "\n",
    "    Parameters:\n",
    "      - useable_root: path with train/dev/test .tsv and source files\n",
    "      - out_root: path where your converter wrote per-language folders\n",
    "      - filter_language: optional filter (applies to prefix or mapped prefix)\n",
    "      - covost_mode: if True, use CoVoST-aware source lookup (first src component = lang code)\n",
    "      - lang_map: optional dict mapping src language codes -> out folder names, e.g. {\"en\": \"english\"}\n",
    "      - recursive_search_depth: how deep (in folder levels) to look under useable_root/lang_code (0 = disable)\n",
    "      - max_workers, show_progress: same as before\n",
    "\n",
    "    Returns:\n",
    "      {\"groups\": { (prefix,split): {...} }, \"summary\": {...} }\n",
    "    \"\"\"\n",
    "    useable_root_p = Path(useable_root).absolute()\n",
    "    out_root_p = Path(out_root).absolute()\n",
    "\n",
    "    # build tasks list\n",
    "    tasks = []\n",
    "    prefixes = set()\n",
    "    for split in SPLITS:\n",
    "        for row in _read_tsv_rows(useable_root_p, split):\n",
    "            # prefix detection: in covost_mode, prefer first path part; otherwise use previous heuristics\n",
    "            p = Path(row[\"src_audio\"])\n",
    "            if covost_mode and len(p.parts) >= 1 and p.parts[0]:\n",
    "                pref = p.parts[0]\n",
    "            elif len(p.parts) > 1:\n",
    "                pref = p.parts[0]\n",
    "            else:\n",
    "                pref = row.get(\"language\") or \"unknown\"\n",
    "            prefixes.add(pref)\n",
    "            tasks.append((pref, split, row))\n",
    "\n",
    "    # filter tasks if requested (match against prefix or mapped name)\n",
    "    if filter_language:\n",
    "        def _match(pref):\n",
    "            mapped = lang_map.get(pref, pref) if lang_map else pref\n",
    "            pref_check = pref == filter_language or pref.replace(\"_\",\"-\") == filter_language or (filter_language in pref)\n",
    "            mapped_check = mapped == filter_language or mapped.replace(\"_\",\"-\") == filter_language or (filter_language in mapped)\n",
    "            return pref_check or mapped_check\n",
    "        tasks = [t for t in tasks if _match(t[0])]\n",
    "        prefixes = {t[0] for t in tasks}\n",
    "        if not tasks:\n",
    "            return {\"error\": f\"no tasks after applying filter_language='{filter_language}'\", \"groups\": {}}\n",
    "\n",
    "    total = len(tasks)\n",
    "    if total == 0:\n",
    "        return {\"error\": \"no tasks found — check useable_root .tsv files\", \"groups\": {}}\n",
    "\n",
    "    # init aggregation buckets\n",
    "    groups = {}\n",
    "    for p in prefixes:\n",
    "        for s in SPLITS:\n",
    "            groups[(p,s)] = {\n",
    "                \"prefix\": p, \"split\": s, \"expected_rows\": 0, \"found_on_dest\": 0,\n",
    "                \"missing_dest\": [], \"missing_src\": [], \"duplicates_counter\": Counter()\n",
    "            }\n",
    "    for pref, split, row in tasks:\n",
    "        key = (pref, split)\n",
    "        groups[key][\"expected_rows\"] += 1\n",
    "        basename = Path(row[\"src_audio\"]).name\n",
    "        groups[key][\"duplicates_counter\"][basename] += 1\n",
    "\n",
    "    # stream-check via executor.map (chunksize tuned for many small tasks)\n",
    "    def _map_iter():\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            for r in ex.map(lambda t: _worker_covost_task(t, useable_root_p, out_root_p, covost_mode, lang_map, recursive_search_depth), tasks, chunksize=256):\n",
    "                yield r\n",
    "\n",
    "    iterator = _map_iter()\n",
    "    if show_progress:\n",
    "        iterator = tqdm(iterator, total=total, desc=\"verifying files\", unit=\"file\")\n",
    "\n",
    "    for res in iterator:\n",
    "        key = (res[\"prefix\"], res[\"split\"])\n",
    "        g = groups[key]\n",
    "        if res[\"dest_exists\"]:\n",
    "            g[\"found_on_dest\"] += 1\n",
    "        else:\n",
    "            if len(g[\"missing_dest\"]) < 200:\n",
    "                g[\"missing_dest\"].append({\"basename\": res[\"basename\"], \"expected_dest\": res[\"dest_path\"], \"src_exists\": res[\"src_exists\"], \"src_tried\": res[\"src_tried\"]})\n",
    "        if not res[\"src_exists\"]:\n",
    "            if len(g[\"missing_src\"]) < 200:\n",
    "                g[\"missing_src\"].append({\"src_field\": res[\"src_field\"], \"tried\": res[\"src_tried\"]})\n",
    "\n",
    "    # finalize\n",
    "    final_groups = {}\n",
    "    total_expected = total_found = total_missing_dest = total_missing_src = 0\n",
    "    groups_with_problems = 0\n",
    "    for key,g in groups.items():\n",
    "        dup_list = [b for b,c in g[\"duplicates_counter\"].items() if c > 1]\n",
    "        missing_dest_count = g[\"expected_rows\"] - g[\"found_on_dest\"]\n",
    "        missing_src_count = len(g[\"missing_src\"])\n",
    "        total_expected += g[\"expected_rows\"]\n",
    "        total_found += g[\"found_on_dest\"]\n",
    "        total_missing_dest += missing_dest_count\n",
    "        total_missing_src += missing_src_count\n",
    "        if missing_dest_count or missing_src_count or dup_list:\n",
    "            groups_with_problems += 1\n",
    "        final_groups[key] = {\n",
    "            \"prefix\": g[\"prefix\"],\n",
    "            \"split\": g[\"split\"],\n",
    "            \"expected_rows\": g[\"expected_rows\"],\n",
    "            \"found_on_dest\": g[\"found_on_dest\"],\n",
    "            \"missing_dest_count\": missing_dest_count,\n",
    "            \"missing_dest_sample\": g[\"missing_dest\"],\n",
    "            \"missing_src_count\": missing_src_count,\n",
    "            \"missing_src_sample\": g[\"missing_src\"],\n",
    "            \"duplicates_count\": len(dup_list),\n",
    "            \"duplicates_sample\": dup_list[:50],\n",
    "        }\n",
    "\n",
    "    summary = {\n",
    "        \"useable_root\": str(useable_root_p),\n",
    "        \"out_root\": str(out_root_p),\n",
    "        \"covost_mode\": covost_mode,\n",
    "        \"lang_map_used\": bool(lang_map),\n",
    "        \"groups_checked\": len(final_groups),\n",
    "        \"total_expected_rows\": total_expected,\n",
    "        \"total_found_on_dest\": total_found,\n",
    "        \"total_missing_on_dest\": total_missing_dest,\n",
    "        \"total_missing_sources\": total_missing_src,\n",
    "        \"groups_with_problems\": groups_with_problems,\n",
    "    }\n",
    "\n",
    "    # short human summary\n",
    "    print(\"==== verify_cvss_copy_covost summary ====\")\n",
    "    print(f\"Total files checked: {total}\")\n",
    "    print(f\"Expected: {total_expected}, found on dest: {total_found}, missing on dest: {total_missing_dest}\")\n",
    "    print(f\"Missing sources: {total_missing_src}\")\n",
    "    if summary[\"groups_with_problems\"] == 0:\n",
    "        print(\"All good ✅\")\n",
    "    else:\n",
    "        print(f\"Issues in {summary['groups_with_problems']} group(s). Inspect results['groups'] for details.\")\n",
    "\n",
    "    return {\"groups\": final_groups, \"summary\": summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = verify_cvss_copy_covost(\n",
    "    useable_root=\"/mnt/covost/orig\",\n",
    "    out_root=\"/mnt/out/cvss-c\",\n",
    "    covost_mode=True,\n",
    "    max_workers=12,\n",
    "    recursive_search_depth=2\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
