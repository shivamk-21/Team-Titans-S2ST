{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32C2i8MwHYCS"
      },
      "source": [
        "##Imports and Logins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwnY0yW_OiXb",
        "outputId": "9c048fab-6020-4e78-9d99-8d7d19fc5993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ri-wjk7MZSl"
      },
      "outputs": [],
      "source": [
        "!pip install librosa soundfile datasets torchcodec huggingface_hub[cli] -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWt_70ICVh4i"
      },
      "outputs": [],
      "source": [
        "HF_token = \"TOKEN\"\n",
        "!export HF_token=TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfqLoIXJQbia"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=HF_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlG1eBBiMwuk"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# bhasaanuvaad = load_dataset(\"ai4bharat/NPTEL\", \"indic2en\", split=\"hindi\", streaming=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QBjY3czHc_5"
      },
      "source": [
        "## Urls for Paraquets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mG4By1cQOt6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = \"https://huggingface.co/api/datasets/ai4bharat/NPTEL/parquet/indic2en/marathi\"\n",
        "\n",
        "# Make request\n",
        "resp = requests.get(url, headers={\"Authorization\": f\"Bearer {HF_token}\"})\n",
        "resp.raise_for_status()  # ensure no error\n",
        "\n",
        "# Parse JSON\n",
        "urls = resp.json()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRzLqjuKc1ah"
      },
      "outputs": [],
      "source": [
        "urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvYwvc6FObYZ"
      },
      "source": [
        "## Select only 3 files at a Time due to RAM constraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhkNJJNDOfSw"
      },
      "outputs": [],
      "source": [
        "urls=urls[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBZHCxqBHl-R"
      },
      "source": [
        "## Downloading Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDxICIugYXwI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def download_parquet_files(url_list, token, output_dir=\"parquets\"):\n",
        "    \"\"\"\n",
        "    Downloads multiple parquet files from given URLs using Hugging Face token.\n",
        "\n",
        "    Args:\n",
        "        url_list (list): List of parquet file URLs.\n",
        "        token (str): Hugging Face API token.\n",
        "        output_dir (str): Directory to save downloaded files.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for url in url_list:\n",
        "        filename = url.split(\"/\")[-1]  # get filename from URL (e.g. 1.parquet)\n",
        "        out_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        # Skip if already downloaded\n",
        "        if os.path.exists(out_path):\n",
        "            print(f\"Skipping already downloaded {filename}\")\n",
        "            continue\n",
        "\n",
        "        with requests.get(url, headers={\"Authorization\": f\"Bearer {token}\"}, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            total_size = int(r.headers.get(\"Content-Length\", 0))\n",
        "            with open(out_path, \"wb\") as f, tqdm(\n",
        "                desc=filename,\n",
        "                total=total_size,\n",
        "                unit=\"B\",\n",
        "                unit_scale=True,\n",
        "                unit_divisor=1024,\n",
        "            ) as bar:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        bar.update(len(chunk))\n",
        "        print(f\"✅ Download complete: {filename}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc1Hmu7lYjmz",
        "outputId": "f990c995-d261-49ba-dac6-418165945c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping already downloaded 0.parquet\n",
            "Skipping already downloaded 1.parquet\n",
            "Skipping already downloaded 2.parquet\n"
          ]
        }
      ],
      "source": [
        "download_parquet_files(urls, HF_token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW1Hr-BoHp2B"
      },
      "source": [
        "## Merging DFs in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "Nsds0a7LZ2Ub",
        "outputId": "edd21322-dec2-49a9-b84c-05c85d706f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 0.parquet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 57892/57892 [00:33<00:00, 1751.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 1.parquet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 57892/57892 [01:01<00:00, 934.09it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 2.parquet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 57892/57892 [02:11<00:00, 439.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged_chunks/merged_batch_0.csv with 173676 rows\n",
            "Loading 3.parquet...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 4447/57892 [00:08<01:38, 542.94it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3110035601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mmerge_parquet_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"chunked_audio_filepath\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"en_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3110035601.py\u001b[0m in \u001b[0;36mmerge_parquet_files\u001b[0;34m(parquet_folder, output_folder, audio_folder, selected_columns, chunk_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0;31m# Save file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_out_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "def merge_parquet_files(parquet_folder=\"parquets\",\n",
        "                        output_folder=\"merged_chunks\",\n",
        "                        audio_folder=\"audio_files\",\n",
        "                        selected_columns=None,\n",
        "                        chunk_size=3):\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    os.makedirs(audio_folder, exist_ok=True)\n",
        "\n",
        "    batch = []\n",
        "    batch_count = 0\n",
        "    file_count = 0\n",
        "    audio_index = 0  # unique counter for audio filenames\n",
        "\n",
        "    for filename in sorted(os.listdir(parquet_folder)):\n",
        "        if filename.endswith(\".parquet\"):\n",
        "            file_count += 1\n",
        "            file_path = os.path.join(parquet_folder, filename)\n",
        "            print(f\"Loading {filename}...\")\n",
        "\n",
        "            # Load only needed columns\n",
        "            df = pd.read_parquet(file_path, columns=selected_columns)\n",
        "\n",
        "            # Process audio column\n",
        "            if \"chunked_audio_filepath\" in df.columns:\n",
        "                audio_paths = []\n",
        "                for _, row in tqdm(df.iterrows(),total=57892):\n",
        "                    if isinstance(row[\"chunked_audio_filepath\"], dict) and \"bytes\" in row[\"chunked_audio_filepath\"]:\n",
        "                        audio_bytes = row[\"chunked_audio_filepath\"][\"bytes\"]\n",
        "                        audio_filename = f\"audio_{batch_count}_{audio_index}.wav\"\n",
        "                        audio_index += 1\n",
        "                        audio_out_path = os.path.join(audio_folder, audio_filename)\n",
        "\n",
        "                        # Save file\n",
        "                        with open(audio_out_path, \"wb\") as f:\n",
        "                            f.write(audio_bytes)\n",
        "\n",
        "                        audio_paths.append(audio_out_path)\n",
        "                    else:\n",
        "                        audio_paths.append(None)\n",
        "\n",
        "                df = df.drop(columns=[\"chunked_audio_filepath\"])\n",
        "                df[\"audio_filepath\"] = audio_paths\n",
        "\n",
        "            batch.append(df)\n",
        "\n",
        "            # Merge and save every chunk_size\n",
        "            if file_count % chunk_size == 0:\n",
        "                merged = pd.concat(batch, ignore_index=True)\n",
        "\n",
        "                out_file = os.path.join(output_folder, f\"merged_batch_{batch_count}.csv\")\n",
        "                merged.to_csv(out_file, index=False)\n",
        "                print(f\"Saved {out_file} with {len(merged)} rows\")\n",
        "\n",
        "                # Free memory explicitly\n",
        "                for b in batch:\n",
        "                    del b\n",
        "                del merged\n",
        "                batch.clear()\n",
        "                gc.collect()\n",
        "\n",
        "                batch_count += 1\n",
        "\n",
        "    # Handle leftovers\n",
        "    if batch:\n",
        "        merged = pd.concat(batch, ignore_index=True)\n",
        "        out_file = os.path.join(output_folder, f\"merged_batch_{batch_count}.csv\")\n",
        "        merged.to_csv(out_file, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"Saved {out_file} with {len(merged)} rows\")\n",
        "\n",
        "        for b in batch:\n",
        "            del b\n",
        "        del merged\n",
        "        batch.clear()\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"✅ Done. Created {batch_count + 1} merged CSV files in {output_folder}\")\n",
        "\n",
        "\n",
        "\n",
        "merge_parquet_files(selected_columns=[\"text\",\"chunked_audio_filepath\",\"en_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3_E75xHPQw4",
        "outputId": "601188db-a4ab-45c8-93a7-de7acb77c5fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          7.45G  99%    1.63MB/s    1:12:29 (xfr#231246, to-chk=0/231569)\n",
            "              0   0%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/3)\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p drive/MyDrive/S2S/hindi\n",
        "!rsync -ah --info=progress2 audio_files/ drive/MyDrive/S2S/hindi/audio_files/\n",
        "!rsync -ah --info=progress2 merged_chunks/ drive/MyDrive/S2S/hindi/merged_chunks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1llnie3WRM9w",
        "outputId": "c26cca2f-161f-4f05-c573-e62ef3e77308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "# !rm -rf drive/MyDrive/S2S/hindi/audio_files\n",
        "!rm -rf drive/MyDrive/S2S/hindi/merged_chunks\n",
        "!rm -rf drive/MyDrive/S2S/hindi/paraquets\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}