{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BXupa1lmzaa"
      },
      "source": [
        "## Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37OPPr0amsS7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQvCHf28mxGo"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9GdJYAVQxBO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBORFJXznK-Z"
      },
      "outputs": [],
      "source": [
        "userName=\"Shivam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haj_tZCebXNa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f'/content/drive/MyDrive/S2S2/WorkSplit/{userName}.csv')\n",
        "df = df.sort_values(by=['lang', 'text']).reset_index(drop=True)\n",
        "\n",
        "total_rows = len(df)\n",
        "total_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nOzI_BMbmZU"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN1-i_rndaua"
      },
      "source": [
        "## Stratified Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuIGWmP7cqD7"
      },
      "outputs": [],
      "source": [
        "lang_proportions = df['lang'].value_counts(normalize=True)\n",
        "display(lang_proportions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zMkTjyEdH7T"
      },
      "outputs": [],
      "source": [
        "num_chunks = 100\n",
        "total_rows = len(df)\n",
        "rows_per_chunk = total_rows // num_chunks\n",
        "\n",
        "rows_per_language_per_chunk = {\n",
        "    lang: round(prop * rows_per_chunk)\n",
        "    for lang, prop in lang_proportions.items()\n",
        "}\n",
        "\n",
        "print(f\"Total rows: {total_rows}\")\n",
        "print(f\"Rows per chunk (approx): {rows_per_chunk}\")\n",
        "print(f\"Rows per language per chunk:\")\n",
        "for lang, count in rows_per_language_per_chunk.items():\n",
        "    print(f\" - {lang}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGA6NE7dc7ka"
      },
      "outputs": [],
      "source": [
        "chunked_dfs = []\n",
        "remaining_df = df.copy()\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "for chunk_id in range(num_chunks):\n",
        "    current_chunk_rows = []\n",
        "\n",
        "    for lang, count in rows_per_language_per_chunk.items():\n",
        "        lang_df = remaining_df[remaining_df['lang'] == lang]\n",
        "        if len(lang_df) >= count:\n",
        "            sampled_rows = lang_df.sample(n=count, replace=False, random_state=RANDOM_SEED + chunk_id)\n",
        "            current_chunk_rows.append(sampled_rows)\n",
        "            remaining_df = remaining_df.drop(sampled_rows.index)\n",
        "        else:\n",
        "            current_chunk_rows.append(lang_df)\n",
        "            remaining_df = remaining_df.drop(lang_df.index)\n",
        "\n",
        "    if current_chunk_rows:\n",
        "        chunked_df = pd.concat(current_chunk_rows)\n",
        "        chunked_dfs.append(chunked_df.reset_index(drop=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX2n747xc-_L"
      },
      "outputs": [],
      "source": [
        "#Sanity Check\n",
        "import random\n",
        "for i, chunk in random.sample(list(enumerate(chunked_dfs)),5):\n",
        "    display(chunk['lang'].value_counts(normalize=True))\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZg-88cNd5DK"
      },
      "source": [
        "## TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5RQA2FuhMcq"
      },
      "outputs": [],
      "source": [
        "!pip install coqui-tts -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr-cPs1zHvPA"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y espeak-ng\n",
        "# !pip install TTS==0.21.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9BW1gWHDFB1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_53Dtaikwc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "base_output = f\"/content/drive/MyDrive/S2S2/TTS_New/{userName}\"\n",
        "langs = df[\"lang\"].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErwsANtlGlYP"
      },
      "outputs": [],
      "source": [
        "from TTS.api import TTS\n",
        "coqui_tts = TTS(model_name=\"tts_models/en/vctk/vits\", progress_bar=False, gpu=True)\n",
        "\n",
        "speakers = coqui_tts.speakers\n",
        "print(f\"Loaded {len(speakers)} voices. Sample: {speakers[:10]}\")\n",
        "\n",
        "selected_speakers = random.sample(speakers, min(10, len(speakers)))  # use 10 random voices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqCM_2oYmi9p"
      },
      "outputs": [],
      "source": [
        "start_chunk=74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sVXJ4VBhQvw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import asyncio\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "MAX_WORKERS = 16\n",
        "MAX_RETRIES = 3\n",
        "RETRY_DELAY = 2\n",
        "CHUNK_PAUSE = 2\n",
        "MIN_FILE_SIZE = 2000\n",
        "\n",
        "def ensure_valid_audio(path):\n",
        "    return os.path.exists(path) and os.path.getsize(path) > MIN_FILE_SIZE\n",
        "\n",
        "\n",
        "def process_row(idx, row):\n",
        "    text = str(row.get(\"en_text\", \"\")).strip()\n",
        "    out_path = os.path.join(base_output, row[\"english_audio_filepath\"].lstrip(\"/\"))\n",
        "\n",
        "    if not text or not out_path:\n",
        "        return f\"Invalid input on row {idx}\"\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            speaker = random.choice(selected_speakers)\n",
        "            coqui_tts.tts_to_file(\n",
        "                text=text,\n",
        "                speaker=speaker,\n",
        "                file_path=out_path\n",
        "            )\n",
        "\n",
        "            if ensure_valid_audio(out_path):\n",
        "                return None\n",
        "            else:\n",
        "                raise ValueError(\"Empty or invalid audio file\")\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < MAX_RETRIES:\n",
        "                time.sleep(RETRY_DELAY)\n",
        "                continue\n",
        "            else:\n",
        "                return \"Failed\"\n",
        "\n",
        "    return None\n",
        "\n",
        "for chunk_idx, chunk in enumerate(chunked_dfs[start_chunk:], start=start_chunk):\n",
        "    print(f\"Processing chunk {chunk_idx+1}/{len(chunked_dfs)}\")\n",
        "\n",
        "    errors = []\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        futures = {executor.submit(process_row, idx, row): idx for idx, row in chunk.iterrows()}\n",
        "\n",
        "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Synthesizing\"):\n",
        "            err = f.result()\n",
        "            if err:\n",
        "                errors.append(err)\n",
        "\n",
        "    if errors:\n",
        "        print(f\"{len(errors)} errors in chunk {chunk_idx+1}:\")\n",
        "        for e in errors[:5]:\n",
        "            print(\"   \", e)\n",
        "\n",
        "    print(f\"Finished chunk {chunk_idx+1}\\n\")\n",
        "    time.sleep(CHUNK_PAUSE)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}